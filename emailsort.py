# -*- coding: utf-8 -*-
"""EmailSort.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ewtjgR95IU7-E7j_LWeqGcGR5LhFwS-M

# **Uploading the datset**
"""

import pandas as pd

data = pd.read_csv("combined_emails_classes.csv",encoding = "latin1") # this dataset is latin1 encoding based

data.head()

"""# **Exploratory Data Analysis**"""

data.shape

data.info()

data.describe()

data.isnull().sum() # looking for total nan values in all rows

"""# **Data Cleaning Process**

## Drop the unwanted columns
"""

data.drop(columns=["Unnamed: 4","Unnamed: 5", "Unnamed: 6", "Unnamed: 7", "Unnamed: 8", "Unnamed: 9"], inplace=True)

"""## Drop all the NaN values in all rows"""

data = data.dropna()

data.head()

data.shape

data.describe()

print(data["email_category"].unique())
print(data['email_category'].value_counts())

print(data["urgency level"].unique())
print(data['urgency level'].value_counts())

"""# Data Preprocessing"""

import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

stop_words = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()

def clean_email(text):
    # Lowercase
    text = text.lower()
    # Remove email IDs
    text = re.sub(r'\S+@\S+', "", text)
    # Remove URLs
    text = re.sub(r"http\S+|www\S+", "", text)
    # Remove punctuation
    text = text.translate(str.maketrans("", "", string.punctuation))
    # Tokenize
    words = text.split()
    # Remove stopwords + lemmatize
    processed_words = [lemmatizer.lemmatize(w) for w in words if w not in stop_words]
    return " ".join(processed_words)

cleaned_subjects = [clean_email(e) for e in data["email_subject"]]
data["email_subject"] = cleaned_subjects

cleaned_body = [clean_email(e) for e in data['email_body']]
data['email_body'] = cleaned_body

data.head()

"""## Encoding the email category and urgency level"""

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()

data['email_category'] = le.fit_transform(data['email_category'])
data['urgency level'] = le.fit_transform(data['urgency level'])

data.head()

data.to_excel("cleaned_email_dataset.xlsx", index = False) # downloading the cleaned email data as an excel file

data

X = data.iloc[:,:2] # input features

y1 = data.iloc[:,2] # first output - classification
y2 = data.iloc[:,3] # second output - urgency level

X

from sklearn.feature_extraction.text import TfidfVectorizer

# combine two features into one string per row
combined_text = data['email_subject'].fillna("") + " " + data['email_subject'].fillna("")

# single string row will be given as as input to TfidVectorizer
vectorizer = TfidfVectorizer(max_features  = 5000,
                             min_df = 7,
                             max_df = 0.2,
                             ngram_range = (1,2))

X = vectorizer.fit_transform(combined_text).toarray()

X

