# EmailSort: Smart Email Classifier for Enterprises 

## ğŸ“Œ Project Overview
This project builds a machine learning pipeline to classify emails into categories and predict their urgency level. The system uses text features from email subject lines and bodies, converts them into numerical representations using TFâ€‘IDF, and trains classifiers to predict both category and urgency.

## ğŸ“‚ Dataset Description
The dataset contains the following fields:

- **email_subject** â†’ short subject line of the email  
- **email_body** â†’ full text content of the email  
- **email_category** â†’ target label for classification  
  - Possible values: `spam`, `inquiry`, `complaint`, `feedback`  
- **urgency level** â†’ target label for urgency  
  - Possible values: `low`, `medium`, `high`

---

## ğŸ“ Data Preparation
- Input features: `email_subject` and `email_body`  
- Output labels:  
  - `y1` â†’ email category (`spam`, `inquiry`, `complaint`, `feedback`)  
  - `y2` â†’ urgency level (`low`, `medium`, `high`)  
- The subject and body are combined into a single text string per row to capture context from both.

---

## ğŸ”  Text Vectorization
The combined text is transformed into numerical features using **TFâ€‘IDF (Term Frequencyâ€“Inverse Document Frequency)**.  
This highlights important words in each email by balancing term frequency with rarity across the dataset.

Key aspects of the vectorization process:
- Limit vocabulary size for efficiency.  
- Ignore extremely rare words.  
- Ignore overly common words.  
- Include both single words (unigrams) and pairs of words (bigrams).

## **Email Category Classification Part**

## Splitting Data into Training and Testing Set for the Model Training

  Several models were trained to classify emails into categories 
  The pipeline explored both traditional machine learning algorithms and modern transformerâ€‘based architectures:
  - **Logistic Regression** â†’ a linear baseline classifier using TFâ€‘IDF features.
  - **Multinomial NaÃ¯ve Bayes** â†’ a probabilistic baseline model wellâ€‘suited for text classification.
  - **BERT (bertâ€‘baseâ€‘uncased)** â†’ a transformer model fineâ€‘tuned for email classification.
  - **DistilBERT** â†’ a lighter, faster version of BERT fineâ€‘tuned for the same task

Each model was evaluated on the same dataset split to ensure fair comparison.

## Evaluation Metrics

  Performance was measured using:
  - **Accuracy** â†’ overall percentage of correctly classified emails.
  - **Classification Report** â†’ detailed perâ€‘class metrics including precision, recall, and F1â€‘score.


## Accuracy Score for Each models

 - DistilBERT :       97.39 %    
 - BERT       :       95.65 %     
 - Logistic Regression :  90.0 %     
 - Multinomial Naive Bayes : 82.0 %      



## ğŸš€ Key Insights
- Transformerâ€‘based models (BERT, DistilBERT) significantly outperform traditional ML baselines.
- DistilBERT achieves the best tradeâ€‘off between accuracy and efficiency, making it ideal for enterprise deployment.
- Logistic Regression provides a strong baseline with lower computational cost.
- NaÃ¯ve Bayes, while fast, struggles with nuanced language patterns in enterprise emails.




---
